{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import tarfile\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "7Q-xj0CcOxJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "DNusgJh7SU53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7910d4ef-69d3-43d4-d5a4-566b6c724532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mexa_hackathon_path = '/content/drive/MyDrive/MEXA_Hackathon'\n",
        "# os.makedirs(mexa_hackathon_path)"
      ],
      "metadata": {
        "id": "_RBSH_D6VOtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/MEXA_Hackathon/FER2013/train.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/drive/MyDrive/MEXA_Hackathon/FER2013\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "pVFOPJmsqsy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV58eTsgOWpe",
        "outputId": "6f2c15db-3a0e-4ed3-df12-e1cc678594a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle username: adrientchuemtchuente\n",
            "Kaggle API key: abf324cba1397099075776e9bbfc77c3\n",
            "Preparing dataset..\n"
          ]
        }
      ],
      "source": [
        "# Set up Kaggle API credentials\n",
        "kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "\n",
        "if not os.path.isfile(kaggle_json_path):\n",
        "    username = input(\"Kaggle username: \")\n",
        "    api_key = input(\"Kaggle API key: \")\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(kaggle_json_path), exist_ok=True)\n",
        "\n",
        "    # Write the credentials to the kaggle.json file\n",
        "    with open(kaggle_json_path, \"w\") as file:\n",
        "        file.write(f'{{\"username\":\"{username}\",\"key\":\"{api_key}\"}}')\n",
        "\n",
        "    # Set file permissions to read and write for the owner only\n",
        "    os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "# Importing kaggle will authenticate automatically\n",
        "import kaggle\n",
        "\n",
        "# Command to authenticate and download the dataset\n",
        "api_command = \"kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge -f fer2013.tar.gz\"\n",
        "\n",
        "# Execute the command\n",
        "try:\n",
        "    subprocess.run(api_command, shell=True, check=True)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\n",
        "        \"An error occurred while downloading the dataset. Please double-check your Kaggle API key.\"\n",
        "    )\n",
        "    os.remove(kaggle_json_path)\n",
        "\n",
        "print(\"Preparing dataset..\")\n",
        "fer2013_zip_path = os.path.join(mexa_hackathon_path, \"fer2013.tar.gz\")\n",
        "fer2013_path = os.path.join(mexa_hackathon_path, \"fer2013\")\n",
        "\n",
        "shutil.move(\"fer2013.tar.gz\", fer2013_zip_path)\n",
        "\n",
        "# Extract the tar.gz file\n",
        "with tarfile.open(fer2013_zip_path, \"r\") as tar:\n",
        "    tar.extractall(fer2013_path)\n",
        "\n",
        "output_folder_path = os.path.join(mexa_hackathon_path, \"FER2013\")\n",
        "\n",
        "# Load the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(os.path.join(fer2013_path, \"fer2013/fer2013.csv\"))\n",
        "\n",
        "# Define a dictionary to map emotion codes to labels\n",
        "emotion_labels = {\n",
        "    \"0\": \"Angry\",\n",
        "    \"1\": \"Disgust\",\n",
        "    \"2\": \"Fear\",\n",
        "    \"3\": \"Happy\",\n",
        "    \"4\": \"Sad\",\n",
        "    \"5\": \"Surprise\",\n",
        "    \"6\": \"Neutral\",\n",
        "}\n",
        "\n",
        "# Create the output folders and subfolders if they do not exist\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "for usage in [\"train\", \"val\", \"test\"]:\n",
        "    usage_folder_path = os.path.join(output_folder_path, usage)\n",
        "    if not os.path.exists(usage_folder_path):\n",
        "        os.makedirs(usage_folder_path)\n",
        "    for label in emotion_labels.values():\n",
        "        subfolder_path = os.path.join(usage_folder_path, label)\n",
        "        if not os.path.exists(subfolder_path):\n",
        "            os.makedirs(subfolder_path)\n",
        "\n",
        "# Loop over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    # Extract the image data from the row\n",
        "    pixels = row[\"pixels\"].split()\n",
        "    img_data = [int(pixel) for pixel in pixels]\n",
        "    img_array = np.array(img_data).reshape(48, 48)\n",
        "    img = Image.fromarray(img_array.astype(\"uint8\"), \"L\")\n",
        "\n",
        "    # Get the emotion label and determine the output subfolder based on the Usage column\n",
        "    emotion_label = emotion_labels[str(row[\"emotion\"])]\n",
        "    if row[\"Usage\"] == \"Training\":\n",
        "        output_subfolder_path = os.path.join(output_folder_path, \"train\", emotion_label)\n",
        "    elif row[\"Usage\"] == \"PublicTest\":\n",
        "        output_subfolder_path = os.path.join(output_folder_path, \"val\", emotion_label)\n",
        "    else:\n",
        "        output_subfolder_path = os.path.join(output_folder_path, \"test\", emotion_label)\n",
        "\n",
        "    # Save the image to the output subfolder\n",
        "    output_file_path = os.path.join(output_subfolder_path, f\"{index}.jpg\")\n",
        "    img.save(output_file_path)\n",
        "\n",
        "# print(\"Deleting temporary files..\")\n",
        "# os.remove(\"fer2013.tar.gz\")\n",
        "# shutil.rmtree(\"fer2013\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
        "from torchvision.ops import StochasticDepth\n",
        "\n",
        "model_urls = {\n",
        "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
        "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
        "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
        "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
        "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
        "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
        "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
        "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
        "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
        "}\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class DotProductSelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(DotProductSelfAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        scale = 1 / math.sqrt(math.sqrt(self.input_dim))\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) * scale\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        attended_values = torch.matmul(attention_weights, value)\n",
        "        output = attended_values + x\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(\n",
        "                x, self.normalized_shape, self.weight, self.bias, self.eps\n",
        "            )\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    r\"\"\"ConvNeXt Block. There are two equivalent implementations:\n",
        "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
        "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
        "    We use (2) as we find it slightly faster in PyTorch\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, drop_path=0.0, layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(\n",
        "            dim, dim, kernel_size=7, padding=3, groups=dim\n",
        "        )  # depthwise conv\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(\n",
        "            dim, 4 * dim\n",
        "        )  # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = (\n",
        "            nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
        "            if layer_scale_init_value > 0\n",
        "            else None\n",
        "        )\n",
        "        self.stochastic_depth = StochasticDepth(drop_path, \"row\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        x = input + self.stochastic_depth(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EmoNeXt(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_chans=3,\n",
        "        num_classes=1000,\n",
        "        depths=None,\n",
        "        dims=None,\n",
        "        drop_path_rate=0.0,\n",
        "        layer_scale_init_value=1e-6,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if dims is None:\n",
        "            dims = [96, 192, 384, 768]\n",
        "        if depths is None:\n",
        "            depths = [3, 3, 9, 3]\n",
        "\n",
        "        # Spatial transformer localization-network\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=7),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(10 * 52 * 52, 32), nn.ReLU(True), nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "\n",
        "        self.downsample_layers = (\n",
        "            nn.ModuleList()\n",
        "        )  # stem and 3 intermediate downsampling conv layers\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
        "                SELayer(dims[i + 1]),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = (\n",
        "            nn.ModuleList()\n",
        "        )  # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[\n",
        "                    Block(\n",
        "                        dim=dims[i],\n",
        "                        drop_path=dp_rates[cur + j],\n",
        "                        layer_scale_init_value=layer_scale_init_value,\n",
        "                    )\n",
        "                    for j in range(depths[i])\n",
        "                ]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
        "        self.attention = DotProductSelfAttention(dims[-1])\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(\n",
        "            torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 10 * 52 * 52)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size(), align_corners=True)\n",
        "        x = F.grid_sample(x, grid, align_corners=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(\n",
        "            x.mean([-2, -1])\n",
        "        )  # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x = self.stn(x)\n",
        "        x = self.forward_features(x)\n",
        "        _, weights = self.attention(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        if labels is not None:\n",
        "            mean_attention_weight = torch.mean(weights)\n",
        "            attention_loss = torch.mean((weights - mean_attention_weight) ** 2)\n",
        "\n",
        "            loss = F.cross_entropy(logits, labels, label_smoothing=0.2) + attention_loss\n",
        "            return torch.argmax(logits, dim=1), logits, loss\n",
        "\n",
        "        return torch.argmax(logits, dim=1), logits\n",
        "\n",
        "\n",
        "def get_model(num_classes, model_size=\"tiny\", in_22k=False):\n",
        "    if model_size == \"tiny\":\n",
        "        depths = [3, 3, 9, 3]\n",
        "        dims = [96, 192, 384, 768]\n",
        "        url = (\n",
        "            model_urls[\"convnext_tiny_22k\"]\n",
        "            if in_22k\n",
        "            else model_urls[\"convnext_tiny_1k\"]\n",
        "        )\n",
        "    elif model_size == \"small\":\n",
        "        depths = [3, 3, 27, 3]\n",
        "        dims = [96, 192, 384, 768]\n",
        "        url = (\n",
        "            model_urls[\"convnext_small_22k\"]\n",
        "            if in_22k\n",
        "            else model_urls[\"convnext_small_1k\"]\n",
        "        )\n",
        "    elif model_size == \"base\":\n",
        "        depths = [3, 3, 27, 3]\n",
        "        dims = [128, 256, 512, 1024]\n",
        "        url = (\n",
        "            model_urls[\"convnext_base_22k\"]\n",
        "            if in_22k\n",
        "            else model_urls[\"convnext_base_1k\"]\n",
        "        )\n",
        "    elif model_size == \"large\":\n",
        "        depths = [3, 3, 27, 3]\n",
        "        dims = [192, 384, 768, 1536]\n",
        "        url = (\n",
        "            model_urls[\"convnext_large_22k\"]\n",
        "            if in_22k\n",
        "            else model_urls[\"convnext_large_1k\"]\n",
        "        )\n",
        "    else:\n",
        "        depths = [3, 3, 27, 3]\n",
        "        dims = [256, 512, 1024, 2048]\n",
        "        url = model_urls[\"convnext_xlarge_22k\"]\n",
        "\n",
        "    default_num_classes = 1000\n",
        "    if in_22k:\n",
        "        default_num_classes = 21841\n",
        "\n",
        "    net = EmoNeXt(\n",
        "        depths=depths, dims=dims, num_classes=default_num_classes, drop_path_rate=0.1\n",
        "    )\n",
        "\n",
        "    state_dict = load_state_dict_from_url(url=url)\n",
        "    net.load_state_dict(state_dict[\"model\"], strict=False)\n",
        "    net.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "0QFbAgm9PChO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import LRScheduler\n",
        "\n",
        "\n",
        "class CosineAnnealingWithWarmRestartsLR(LRScheduler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        warmup_steps: int = 128,\n",
        "        cycle_steps: int = 512,\n",
        "        min_lr: float = 0.0,\n",
        "        max_lr: float = 1e-3,\n",
        "    ):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.cycle_steps = cycle_steps\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "\n",
        "        self.steps_counter = 0\n",
        "\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        self.steps_counter += 1\n",
        "\n",
        "        current_cycle_steps = self.steps_counter % self.cycle_steps\n",
        "\n",
        "        if current_cycle_steps < self.warmup_steps:\n",
        "            current_lr = (\n",
        "                self.min_lr\n",
        "                + (self.max_lr - self.min_lr) * current_cycle_steps / self.warmup_steps\n",
        "            )\n",
        "        else:\n",
        "            current_lr = (\n",
        "                self.min_lr\n",
        "                + (self.max_lr - self.min_lr)\n",
        "                * (\n",
        "                    1\n",
        "                    + math.cos(\n",
        "                        math.pi\n",
        "                        * (current_cycle_steps - self.warmup_steps)\n",
        "                        / (self.cycle_steps - self.warmup_steps)\n",
        "                    )\n",
        "                )\n",
        "                / 2\n",
        "            )\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = current_lr\n"
      ],
      "metadata": {
        "id": "wYH7s8uEPMAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ema_pytorch"
      ],
      "metadata": {
        "id": "Jzvgj_iLPjlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda46902-68a6-447a-d31e-1a32464ba321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ema_pytorch\n",
            "  Downloading ema_pytorch-0.7.7-py3-none-any.whl.metadata (689 bytes)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ema_pytorch) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema_pytorch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->ema_pytorch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ema_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ema_pytorch) (3.0.2)\n",
            "Downloading ema_pytorch-0.7.7-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ema_pytorch\n",
            "Successfully installed ema_pytorch-0.7.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from ema_pytorch import EMA\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "import wandb\n",
        "#from models import get_model\n",
        "#from scheduler import CosineAnnealingWithWarmRestartsLR\n",
        "\n",
        "seed = 2001\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        training_dataloader,\n",
        "        validation_dataloader,\n",
        "        testing_dataloader,\n",
        "        classes,\n",
        "        output_dir,\n",
        "        max_epochs: int = 10000,\n",
        "        early_stopping_patience: int = 12,\n",
        "        execution_name=None,\n",
        "        lr: float = 1e-4,\n",
        "        amp: bool = False,\n",
        "        ema_decay: float = 0.99,\n",
        "        ema_update_every: int = 16,\n",
        "        gradient_accumulation_steps: int = 1,\n",
        "        checkpoint_path: str = None,\n",
        "    ):\n",
        "        self.epochs = max_epochs\n",
        "\n",
        "        self.training_dataloader = training_dataloader\n",
        "        self.validation_dataloader = validation_dataloader\n",
        "        self.testing_dataloader = testing_dataloader\n",
        "\n",
        "        self.classes = classes\n",
        "        self.num_classes = len(classes)\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Device used: \" + self.device.type)\n",
        "\n",
        "        self.amp = amp\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        self.optimizer = AdamW(model.parameters(), lr=lr)\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
        "        self.scheduler = CosineAnnealingWithWarmRestartsLR(\n",
        "            self.optimizer, warmup_steps=128, cycle_steps=1024\n",
        "        )\n",
        "        self.ema = EMA(model, beta=ema_decay, update_every=ema_update_every).to(\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "\n",
        "        self.output_directory = Path(output_dir)\n",
        "        self.output_directory.mkdir(exist_ok=True)\n",
        "\n",
        "        self.best_val_accuracy = 0\n",
        "\n",
        "        self.execution_name = \"model\" if execution_name is None else execution_name\n",
        "\n",
        "        if checkpoint_path:\n",
        "            self.load(checkpoint_path)\n",
        "\n",
        "        wandb.watch(model, log=\"all\")\n",
        "\n",
        "    def run(self):\n",
        "        counter = 0  # Counter for epochs with no validation loss improvement\n",
        "\n",
        "        images, _ = next(iter(self.training_dataloader))\n",
        "        images = [transforms.ToPILImage()(image) for image in images]\n",
        "        wandb.log({\"Images\": [wandb.Image(image) for image in images]})\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            print(\"[Epoch: %d/%d]\" % (epoch + 1, self.epochs))\n",
        "\n",
        "            self.visualize_stn()\n",
        "            train_loss, train_accuracy = self.train_epoch()\n",
        "            val_loss, val_accuracy = self.val_epoch()\n",
        "\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"Train Loss\": train_loss,\n",
        "                    \"Val Loss\": val_loss,\n",
        "                    \"Train Accuracy\": train_accuracy,\n",
        "                    \"Val Accuracy\": val_accuracy,\n",
        "                    \"Epoch\": epoch + 1,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Early stopping\n",
        "            if val_accuracy > self.best_val_accuracy:\n",
        "                self.save()\n",
        "                counter = 0\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "            else:\n",
        "                counter += 1\n",
        "                if counter >= self.early_stopping_patience:\n",
        "                    print(\n",
        "                        \"Validation loss did not improve for %d epochs. Stopping training.\"\n",
        "                        % self.early_stopping_patience\n",
        "                    )\n",
        "                    break\n",
        "\n",
        "        self.test_model()\n",
        "        wandb.finish()\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "\n",
        "        avg_accuracy = []\n",
        "        avg_loss = []\n",
        "\n",
        "        pbar = tqdm(unit=\"batch\", file=sys.stdout, total=len(self.training_dataloader))\n",
        "        for batch_idx, data in enumerate(self.training_dataloader):\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            with torch.autocast(self.device.type, enabled=self.amp):\n",
        "                predictions, _, loss = self.model(inputs, labels)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.optimizer.zero_grad(set_to_none=True)\n",
        "                self.scaler.update()\n",
        "                self.ema.update()\n",
        "                self.scheduler.step()\n",
        "\n",
        "            batch_accuracy = (predictions == labels).sum().item() / labels.size(0)\n",
        "\n",
        "            avg_loss.append(loss.item())\n",
        "            avg_accuracy.append(batch_accuracy)\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix(\n",
        "                {\"loss\": np.mean(avg_loss), \"acc\": np.mean(avg_accuracy) * 100.0}\n",
        "            )\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        return np.mean(avg_loss), np.mean(avg_accuracy) * 100.0\n",
        "\n",
        "    def val_epoch(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        avg_loss = []\n",
        "        predicted_labels = []\n",
        "        true_labels = []\n",
        "\n",
        "        pbar = tqdm(\n",
        "            unit=\"batch\", file=sys.stdout, total=len(self.validation_dataloader)\n",
        "        )\n",
        "        for batch_idx, (inputs, labels) in enumerate(self.validation_dataloader):\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            with torch.autocast(self.device.type, enabled=self.amp):\n",
        "                predictions, _, loss = self.model(inputs, labels)\n",
        "\n",
        "            avg_loss.append(loss.item())\n",
        "            predicted_labels.extend(predictions.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        accuracy = (\n",
        "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
        "            .float()\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                    probs=None,\n",
        "                    y_true=true_labels,\n",
        "                    preds=predicted_labels,\n",
        "                    class_names=self.classes,\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            \"Eval loss: %.4f, Eval Accuracy: %.4f %%\"\n",
        "            % (np.mean(avg_loss) * 1.0, accuracy * 100.0)\n",
        "        )\n",
        "        return np.mean(avg_loss), accuracy * 100.0\n",
        "\n",
        "    def test_model(self):\n",
        "        self.ema.eval()\n",
        "\n",
        "        predicted_labels = []\n",
        "        true_labels = []\n",
        "\n",
        "        pbar = tqdm(unit=\"batch\", file=sys.stdout, total=len(self.testing_dataloader))\n",
        "        for batch_idx, (inputs, labels) in enumerate(self.testing_dataloader):\n",
        "            bs, ncrops, c, h, w = inputs.shape\n",
        "            inputs = inputs.view(-1, c, h, w)\n",
        "\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            with torch.autocast(self.device.type, enabled=self.amp):\n",
        "                _, logits = self.ema(inputs)\n",
        "            outputs_avg = logits.view(bs, ncrops, -1).mean(1)\n",
        "            predictions = torch.argmax(outputs_avg, dim=1)\n",
        "\n",
        "            predicted_labels.extend(predictions.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        accuracy = (\n",
        "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
        "            .float()\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        print(\"Test Accuracy: %.4f %%\" % (accuracy * 100.0))\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                    probs=None,\n",
        "                    y_true=true_labels,\n",
        "                    preds=predicted_labels,\n",
        "                    class_names=self.classes,\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def visualize_stn(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        batch = torch.utils.data.Subset(val_dataset, range(32))\n",
        "\n",
        "        # Access the batch data\n",
        "        batch = torch.stack([batch[i][0] for i in range(len(batch))]).to(self.device)\n",
        "        with torch.autocast(self.device.type, enabled=self.amp):\n",
        "            stn_batch = self.model.stn(batch)\n",
        "\n",
        "        to_pil = transforms.ToPILImage()\n",
        "\n",
        "        grid = to_pil(torchvision.utils.make_grid(batch, nrow=16, padding=4))\n",
        "        stn_batch = to_pil(torchvision.utils.make_grid(stn_batch, nrow=16, padding=4))\n",
        "\n",
        "        wandb.log({\"batch\": wandb.Image(grid), \"stn\": wandb.Image(stn_batch)})\n",
        "\n",
        "    def save(self):\n",
        "        data = {\n",
        "            \"model\": self.model.state_dict(),\n",
        "            \"opt\": self.optimizer.state_dict(),\n",
        "            \"ema\": self.ema.state_dict(),\n",
        "            \"scaler\": self.scaler.state_dict(),\n",
        "            \"scheduler\": self.scheduler.state_dict(),\n",
        "            \"best_acc\": self.best_val_accuracy,\n",
        "        }\n",
        "\n",
        "        torch.save(data, str(self.output_directory / f\"{self.execution_name}.pt\"))\n",
        "\n",
        "    def load(self, path):\n",
        "        data = torch.load(path, map_location=self.device)\n",
        "\n",
        "        self.model.load_state_dict(data[\"model\"])\n",
        "        self.optimizer.load_state_dict(data[\"opt\"])\n",
        "        self.ema.load_state_dict(data[\"ema\"])\n",
        "        self.scaler.load_state_dict(data[\"scaler\"])\n",
        "        self.scheduler.load_state_dict(data[\"scheduler\"])\n",
        "        self.best_val_accuracy = data[\"best_acc\"]\n",
        "\n",
        "\n",
        "def plot_images():\n",
        "    # Create a grid of images for visualization\n",
        "    num_rows = 4\n",
        "    num_cols = 8\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 5))\n",
        "\n",
        "    # Plot the images\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            index = i * num_cols + j  # Calculate the corresponding index in the dataset\n",
        "            image, _ = train_dataset[index]  # Get the image\n",
        "            axes[i, j].imshow(\n",
        "                image.permute(1, 2, 0)\n",
        "            )  # Convert tensor to PIL image format and plot\n",
        "            axes[i, j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"images.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "AMRnEopHPRmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    model_size = \"tiny\"\n",
        "    exec_name = f\"EmoNeXt_{model_size}_{current_time}\"\n",
        "    dataset_path = os.path.join(mexa_hackathon_path, \"FER2013\")\n",
        "    batch_size = 64\n",
        "    num_workers = 2\n",
        "    in_22k = False\n",
        "    lr = 0.01\n",
        "    output_dir = os.path.join(mexa_hackathon_path, f\"out_{current_time}\")\n",
        "    checkpoint = None\n",
        "    epochs = 40\n",
        "    amp = False # Enable mixed precision training\n",
        "\n",
        "    wandb_dir = os.path.join(mexa_hackathon_path, \"FER2013/wandb\")\n",
        "    wandb.init(project=\"EmoNeXt\", name=exec_name, anonymous=\"must\", dir= wandb_dir)\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize(236),\n",
        "            transforms.RandomRotation(degrees=20),\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize(236),\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    test_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize(236),\n",
        "            transforms.TenCrop(224),\n",
        "            transforms.Lambda(\n",
        "                lambda crops: torch.stack(\n",
        "                    [transforms.ToTensor()(crop) for crop in crops]\n",
        "                )\n",
        "            ),\n",
        "            transforms.Lambda(\n",
        "                lambda crops: torch.stack([crop.repeat(3, 1, 1) for crop in crops])\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(dataset_path + \"/train\", train_transform)\n",
        "    val_dataset = datasets.ImageFolder(dataset_path + \"/val\", val_transform)\n",
        "    test_dataset = datasets.ImageFolder(dataset_path + \"/test\", test_transform)\n",
        "\n",
        "    print(\"Using %d images for training.\" % len(train_dataset))\n",
        "    print(\"Using %d images for evaluation.\" % len(val_dataset))\n",
        "    print(\"Using %d images for testing.\" % len(test_dataset))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    net = get_model(len(train_dataset.classes), model_size, in_22k=in_22k)\n",
        "\n",
        "    Trainer(\n",
        "        model=net,\n",
        "        training_dataloader=train_loader,\n",
        "        validation_dataloader=val_loader,\n",
        "        testing_dataloader=test_loader,\n",
        "        classes=train_dataset.classes,\n",
        "        execution_name=exec_name,\n",
        "        lr=lr,\n",
        "        output_dir=output_dir,\n",
        "        checkpoint_path=checkpoint,\n",
        "        max_epochs=epochs,\n",
        "        amp=amp,\n",
        "    ).run()\n"
      ],
      "metadata": {
        "id": "ixxdPNc8SCzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "8c43824c-4b48-4e34-e50a-85d645eb12fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/MEXA_Hackathon/FER2013/wandb/wandb/run-20250127_133640-w5augwwk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anony-moose-904178181889900919/EmoNeXt/runs/w5augwwk?apiKey=2c3484cc1ca16a823e8da63900703170d82c941e' target=\"_blank\">EmoNeXt_tiny_2025-01-27 13:36:32</a></strong> to <a href='https://wandb.ai/anony-moose-904178181889900919/EmoNeXt?apiKey=2c3484cc1ca16a823e8da63900703170d82c941e' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/anony-moose-904178181889900919/EmoNeXt?apiKey=2c3484cc1ca16a823e8da63900703170d82c941e' target=\"_blank\">https://wandb.ai/anony-moose-904178181889900919/EmoNeXt?apiKey=2c3484cc1ca16a823e8da63900703170d82c941e</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/anony-moose-904178181889900919/EmoNeXt/runs/w5augwwk?apiKey=2c3484cc1ca16a823e8da63900703170d82c941e' target=\"_blank\">https://wandb.ai/anony-moose-904178181889900919/EmoNeXt/runs/w5augwwk?apiKey=2c3484cc1ca16a823e8da63900703170d82c941e</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 28709 images for training.\n",
            "Using 3589 images for evaluation.\n",
            "Using 3589 images for testing.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny_1k_224_ema.pth\n",
            "100%|| 109M/109M [00:00<00:00, 254MB/s] \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device used: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-600f0dbced60>:67: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch: 1/40]\n",
            "100%|| 449/449 [1:41:47<00:00, 13.60s/batch, loss=1.87, acc=25.2]\n",
            "100%|| 3589/3589 [26:44<00:00,  2.24batch/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval loss: 1.8695, Eval Accuracy: 24.9373 %\n",
            "[Epoch: 2/40]\n",
            "100%|| 449/449 [07:03<00:00,  1.06batch/s, loss=1.88, acc=25.1]\n",
            "100%|| 3589/3589 [01:04<00:00, 55.36batch/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval loss: 1.8678, Eval Accuracy: 24.9373 %\n",
            "[Epoch: 3/40]\n",
            "100%|| 449/449 [07:03<00:00,  1.06batch/s, loss=1.88, acc=25]\n",
            "100%|| 3589/3589 [01:04<00:00, 55.96batch/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eval loss: 1.8682, Eval Accuracy: 24.9373 %\n",
            "[Epoch: 4/40]\n",
            "100%|| 449/449 [07:04<00:00,  1.06batch/s, loss=1.88, acc=25.1]\n",
            "100%|| 3589/3589 [01:04<00:00, 55.23batch/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Eval loss: 1.8686, Eval Accuracy: 24.9373 %\n",
            "[Epoch: 5/40]\n",
            "100%|| 449/449 [07:04<00:00,  1.06batch/s, loss=1.88, acc=25.2]\n",
            " 39%|      | 1417/3589 [00:26<01:02, 34.87batch/s]"
          ]
        }
      ]
    }
  ]
}